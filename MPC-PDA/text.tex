\section{Hierarchie výpočetní složitosti. P, P-úplné, NP, NP-úplné, PSPACE, EXP-SPACE, HP těžké, rozhodnutelné, nerozhodnutelné. Definice problému s~batohem. Definice problému k-řezu. Definice problému obchodu.}

\subsection{Polynomická vs exponenciální časová složitost.}

Algoritmy s~polynomickou složitostí lze efektivně řešit, bez vynaložení velkého výpočetního výkonu. 
S~délkou vstupu \textit{n} roste lineárně potřebný čas ($n^x$) například hledání nejkratší cesty. 
Algoritmy s~exponenciální složitostí lze efektivně řešit pouze pro~malé exponenty, bez potřeby velkého výpočetního výkonu. 
S~délkou vstupu \textit{n} roste exponenciálně potřebný čas ($x^n$), například v~problému obchodního cestujícího.

\subsection{Hierarchie výpočetní složitosti.}

% TODO Doplnit třídy ze zadání

Hierarchie výpočetní složitosti slouží k~charakterizaci algoritmů u~kterých lze pouze nepřímo určit asymptotickou složitost. 
Využívá se třídní hierarchie. 
Třídy od nejjednodušší po nejsložitější jdou v~tomto pořadí: P $\subseteq$ NP $\subseteq$ \#P $\subseteq$ PSPACE $\subseteq$ EXP $\subseteq$ NP-složité $\subseteq$ neřešitelné. 
Jednoduší třída je vždy podmnožinou složitější a do~složitější patří i ty jednoduší.

\subsubsection{Třída P}

Třída P obsahuje polynomiální problémy. 
Tyto problémy se dají vyřešit v~polynomiálním čase na~deterministickém Turingově stroji (TS). 
Příkladem může být nalezení nejkratší cesty, minimální kostry v~grafu.

\subsubsection{Třída NP}

Třída NP obsahuje nedeterministické polynomiální problémy. 
Tyto problémy lze řešit v~polynomiálním čase na~nedeterministickém TS (doposud nesestaven).

\subsubsection{Třída NP-complete (NP-úplné)}

NP-úplné (NP-C) je podskupina problémů třídy NP, která se zabývá rozhodovacími problémy.
Jejich řešení je nejtěžší a všechna známá řešení lze na~deterministickém TS provést pouze s~exponenciálním čase. 
Pro~polynomiální nebylo zatím nalezeno řešení, ale zároveň nebylo dokázáno, že řešení neexistuje.

\textbf{Problém NP-C}: pokud by se podařilo převést jeden NP-C problém na~polynomiální čas tak to znamená, že každý NP-C problém lze převést do~polynomiálního. 
To by vedlo k~prokázání P $=$ NP.

Bylo dokázáno, že každý algoritmus pro~NP-C problém lze použít k~řešení jiného \mbox{NP-C} problému.
Přibližně existuje 10\,000 známých NP-C problémů.
Jde napříkald o~barvení grafů, Knapsnack (problém batohu), 3-partition problém (rozdělit množinu čísel na~podmnožiny o~velikosti 3 se stejným součtem), \dots

\subsubsection{Třída NP-těžké}

O problému řekneme, že je NP-težký, jestliže se na~něj redukuje NP-C problém, ale zároveň nevíme jestli spadá do~NP.
Redukce znamená že se dá převést/konvertovat.

\subsubsection{P vs NP}

P~je podmnožina NP, jelikož každý problém lze řešit v~polynomiálním čase na
 nedeterministickém TS.
Existuje matematický problém P $=$ NP, který dosud nebyl potvrzen nebo vyvrácen. Takže se přesně nedá určit jestli je P $=$ NP. 
Obecně se považuje, že P $\neq$ NP.

\subsubsection{Třída \#P}

Třída \#P se nezabývá rozhodovacími problémy, ale řeší problém nalezení možných řešení NP problémů. 
Z toho plyne, že problémy třídy \#P musí být alespoň stejně složité jako stejný NP problém.
U této třídy se neptáme jestli existuje řešení, ale kolik řešení existuje.

\textbf{Definice:} \#P je množina všech funkcí $f(x)$, kde $f(x)$ odpovídá počtu přijatelných cest nedeterministického TS v~polynomiálním čase.

V~této třídě existují také problémy \#P-úplné.

\subsubsection{Třída PSPACE}

PSPACE je množina rozhodovacích problémů, které lze vyřešit pomocí deterministického TS v~polynomiálním paměťovém prostoru.
Neboli nezáleží, jak dlouho trvá vykonání, když je využit polynomiální (rozumné) množství paměťi. 
PSPACE má úplné problémy, které jsou sestupně samo redukovatelné (downward self-reducible) a náhodně samo redukovatelné (random self-reducible).
Jsou zahrnuté v~EXP.
Dle Savitchova theorému můžeme dokázat, že \textbf{P}SPACE = \textbf{NP}SPACE.

\subsubsection{Třída EXP}

Často označována jako EXPTIME.
Je to množina rozhodovacích problémů řešitelných na~deterministickém TS v~exponenciálním čase.
Časová komplexita je O($2^{p(n)}$), kde $p(n)$ je polynomická funkce $n$.
Jejich složitost je exponenciální (například $2^n$).

Existuje také třída EXPSPACE, která je podobná EXPTIME, jen se neřeší v~exponenciálním času, ale v~exponenciálním paměťovém prostoru.

U~obou tříd se jedná o~nezvládnutelné problémy, pro~které neexistuje polynomiální algoritmus.

\subsection{Rozhodnutelnost}

% TODO

\subsection{Problém batohu}

Zloděj má pro~krádeže připravený batoh s~určitou maximální hmotností/velikostí.
Při~krádeži se snaží do~batohu vložit co nejvíce zboží v~co největší hodnotě.
Každé toto zboží má jinou hmotnost/velikost a jinou hodnotu.
Hledá nejlepší kombinaci zboží která se mu do~batohu vejde s~cílem maximalizovat profit.

\subsection{Problém k-řezu}

% TODO

\subsection{Problém obchodního cestujícího}

V~tomto NP-těžkém problému (\emph{Travelling Salesman Problem, TSP}) jde o~nalezení té nejkratší Hamiltonské cesty v~grafu $G$:
Hamiltonská cesta $P$ je taková cesta, která navštíví každý vrchol právě jednou.

Heurestické algoritmy negarantují že nalezly nejlepší řešení, ale operují v~dosažitelném čase.
Příkladem je \href{https://en.wikipedia.org/wiki/Nearest_neighbour_algorithm}{\emph{Nearest Neighbour}} (dochází k~\enquote{slepému} propojování nejbližších bodů bez~znalosti celku), který může vrátit nejlepší i~velmi špatný výsledek.

\subsubsection{Modifikace genetických algoritmů pro~TSP}

Genetické algoritmy nejdou na~TSP aplikovat přímo, protože každý z~vrcholů může být navštíven pouze jednou.

\begin{itemize}
\item přirozený výběr: fitness funkce je celková délka cesty a~nevyžaduje modifikaci
\item mutace: prohození dvou vrcholů uvnitř chromozomu nevyžaduje modifikaci
\item křížení: nestačí vyměnit dvě části chromozomu, je třeba ho \enquote{přefiltrovat} a~vybrat pouze takové vrcholy, které se v~chormozomu ještě neobjevily (a~tyto vyfiltrované nahradit)
\end{itemize}

% \subsection{Problém směrování vozidel (VRP)}
% 
% Centrální sklad má $x$ vozidel.
% Vozidla musí objet všechna zásobovací místa, dovést tam zboží a vrátit se zpět.
% Nemusí navštívit stejný počet míst, ale dohromady musí navštívit všechna místa co nejoptimálněji.
% Pro~$x=1$ jde o~problém obdobný problému obchodního cestujícího, ale je zde navíc určen začátek cesty.
% 
% \subsection{Metrický k-střed}
% 
% $x$ měst má mezi sebou přesně definované vzdálenosti.
% Mezi nimi (resp. do~nějakých z~měst) lze umístit $n$ skladišť která budou všechna města obsluhovat.
% Cílem je vybrat nejvhodnější města pro~postavení skladu, aby maximální vzdálenost každého města ke~skladu byla co nejmenší.

\clearpage
\section{Optimalizace genetickým programováním (inicializace, křížení, mutace). Optimalizace rojem částic. Optimalizace mravenčí kolonií.}

\subsection{Genetické programování}

Genetické programování pracuje se~stromy, ne vektory.
Strom obsahuje pouze funkce a~terminály (které jsou pouze v~listech a nemají potomky).

Náhodně generované programy využívají základní operace a~konstanty pro~rozhodovací strom s~omezenou hloubkou.
Vybraná funkce rekurzivně generuje další stromy.

\subsubsection{Operace}

Stromy se kříží prohozením podstromů.
% TODO

\begin{figure}[ht]
    \centering
    \tikzstyle{node}=[circle, draw, text centered]
    \begin{tikzpicture}
        \node[node](+) at (3, 3) {\texttt{+}};

        \node[node](2) at (0, 2) {\texttt{2}};
        \node[node](3) at (2, 2) {\texttt{3}};
        \node[node](*) at (4, 2) {\texttt{*}};
        \node[node](/) at (6, 2) {\texttt{/}};

        \node[node](x) at (3, 0.5) {\texttt{x}};
        \node[node](7) at (4, 0.5) {\texttt{7}};
        \node[node](y) at (5, 0.5) {\texttt{y}};
        \node[node](5) at (6, 0.5) {\texttt{5}};

        \begin{scope}[every path/.style={-}, every node/.style={inner sep=1pt}]
            \draw (2) -- node {} (+);
            \draw (3) -- node {} (+);
            \draw (*) -- node {} (+);
            \draw (/) -- node {} (+);

            \draw (x) -- node {} (*);
            \draw (7) -- node {} (*);
            \draw (y) -- node {} (/);
            \draw (5) -- node {} (/);
        \end{scope}
    \end{tikzpicture}
    \caption{Strom řešící příklad $5 + 7x + y/5 = 0$}
\end{figure}
\FloatBarrier

\subsection{Optimalizace rojem částic}

Každá částice se snaží aktualizovat svou pozici $X$ v~prostoru (2D, 3D, \dots):

$$X(t+1) = X(t) + V(t+1)$$
$$V(t+1) = WV(t) + C_1 \cdot \mathrm{\texttt{rand()}} X (X_\mathrm{pbest} - X(t)) + C_2 \cdot \mathrm{\texttt{rand()}} X (X_\mathrm{gbest} - X(t))$$
%
kde $V(t)$ je rychlost v~čase $t$, $X(t)$ je pozice v~čase $t$, $W$ je váha, $C_1$/$C_2$ jsou učící a~akcelerační faktory, \texttt{rand()} je reálné číslo $\left<0, 1\right>$, $X_\mathrm{pbest}$ je nejlepší osobní pozice částice a~$X_\mathrm{gbest}$ je globální nejlepší pozice.

Jedna iterace algoritmu, tj. výpočet fitness funkce, probíhá ve~třech krocích:

\begin{enumerate}
\item aktualizace osobního i~globálního nejlepšího výsledku,
\item aktualizace rychlosti částic,
\item aktualizace pozice částic.
\end{enumerate}

\subsection{Optimalizace mravenční kolonií}

Simuluje mnoho nezávislých kooperujících jedinců:

\begin{enumerate}
\item vytvoření částečného řešení,
\item přidání hrany na~základě stochastických parametrů a~feromonů,
\item vyhledání lokálního minima,
\item aktualizace feromonů dobrých řešení.
\end{enumerate}

% \subsection{Evoluční strategie}
% 
% Využívá pouze mutaci, ne křížení.
% Jednotlivci jsou většinou reprezentováni jako pár vektorů reálných čísel: $v = (x, \sigma)$, kde $x$ je souřadnice pozice a~$\sigma$ je vektor směrodatných odchylek v~daném bodu.

\clearpage
\section{Grafy -- incidenční matice, matice sousedností. Handshaking lemma. Silně propojené komponenty grafu. Kosarajův algoritmus, Tarjanův algoritmus.}

Graf je matematická struktura $G = (V, E)$: uspořádaná dvojice množin vrcholů a~hran (\emph{vertices and edges}), kde hrana je určena dvěma vrcholy a~volitelně směrem nebo váhou.
Velké množství problémů postavených nad~grafy je NP-úplných.

\subsection{Maticové reprezentace}

Incidenční matice obsahuje informace o~mapování vrcholů jednotlivým hranám.
Matice má řádek pro~každý vrchol a~sloupec pro každou hranu; pokud vrchol hraně náleží, je na~pozici jednička (pro~orientované grafy může mít výchozí vrchol hodnotu $-1$).

Matice souslednosti má podobu čtvercové matice $n \times n$ (kde $n$ je počet vrcholů grafu), jejíž hodnota na~místě $a_{i,j}$ je celé číslo odpovídající počtu hran vedoucích z~vrcholu $i$ do~vrcholu $j$, prvky na~diagonále pak odpovídají počtu hran vedoucích z~vrcholu $i$ do~vrcholu $i$.

\begin{figure}[ht!]
    \centering
    \begin{minipage}{0.20\textwidth}
        \centering
        \includegraphics[height=8em]{images/3_graf-matice-souslednosti}
        \caption[Ilustrační graf]{Ilustrační graf\\{\small (Chris-martin, Wikimedia Commons, volné dílo)}}
        \label{ilustracni-graf-matice}
    \end{minipage}%
    %
    \begin{minipage}{0.4\textwidth}
        \centering
        $\left( \begin{matrix}
        2 & 1 & 1 & 0 & 0 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 0 & 0 \\
        0 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 0 & 0 & 0 & 1 & 1 & 1 \\
        0 & 0 & 1 & 0 & 0 & 1 & 0 \\
        0 & 0 & 0 & 0 & 0 & 0 & 1 \\
        \end{matrix} \right)$ 
        \caption[Incidenční matice]{Incidenční matice\\(vrcholy a hrany)}
        % https://sciencedirect.com/topics/mathematics/incidence-matrix
    \end{minipage}%
    %
    \begin{minipage}{0.4\textwidth}
        \centering
        $\left( \begin{matrix}
        2 & 1 & 0 & 0 & 1 & 0 \\
        1 & 0 & 1 & 0 & 1 & 0 \\
        0 & 1 & 0 & 1 & 0 & 0 \\
        0 & 0 & 1 & 0 & 1 & 1 \\
        1 & 1 & 0 & 1 & 0 & 0 \\
        0 & 0 & 0 & 1 & 0 & 0 \\
        \end{matrix} \right)$
        \caption[Matice souslednosti]{Matice souslednosti\\(vrcholy k~vrcholům: diagonálně symetrická)}
    \end{minipage}
\end{figure}
\FloatBarrier

\subsection{Handshaking lemma}

Handshaking lemmma je tvrzení že pro~každý konečný neorientovaný graf platí, že počet vrcholů s~lichým stupněm je sudý (formálně $\sum_{v \in V} \deg v = 2 |E|$: součet stupňů vrcholů odpovídá dvojnásobku počtu hran).

V~roce 1736 byla dokázána Leonardem Eulerem v~dokumentu řešícím problém Sedmi mostů města Královce\footnote{\url{https://en.wikipedia.org/wiki/Seven_Bridges_of_Konigsberg}}.
Graf obsahuje Eulerovskou cestu právě tehdy, když jím lze projít tak, aby byla každá hrana navštívena právě jednou.

\subsection{Silně propojené komponenty}

\emph{Graf} je silně propojený tehdy, když je každý vrchol dosažitelný z~každého jiného vrcholu.
Silně propojené komponenty jsou podgrafy, které jsou samy o~sobě pevně propojeny.

\subsubsection{Kosarajův algoritmus}

Algoritmus hledající silně propojené komponenty pracující v~lineárním čase.
% TODO Tato poznámka je sice pravdivá, ale není zřejmá z pseudokódu
% Využívá faktu, že transponovaný graf $G^T$ má stejné silně propojené komponenty jako původní graf $G$.

% TODO ^^ Blbě! https://www.youtube.com/watch?v=Jb1XlDsr46o
\begin{figure}[ht]
\onehalfspacing
\begin{enumerate}
\item Označ každý vrchol grafu za~nenavštívený
\item Vytvoř zásobník $L$
\item Pro každý vrchol grafu $u$ spusť podprogram Navštiv($u$):
    \begin{itemize}
    \item Pokud je $u$ nenavštívený:
        \begin{enumerate}
        \item Označ $u$ jako navštívený
        \item Pro každého souseda $n$ zavolej Navštiv($n$)
        \item Přidej $u$ na~vrchol zásobníku
        \end{enumerate}
    \item Jinak nedělej nic
    \end{itemize}
\item Pro každý vrchol $u$ zásobníku $L$ spusť podprogram Přiřaď($u$, $u$):
    \begin{itemize}
    \item Pokud $u$ nepatří žádné komponentě:
        \begin{enumerate}
        \item Přiřaď $u$ komponentě $A$
        \item Pro každého souseda $n$ zavolej Přiřaď($n$, $u$)
        \end{enumerate}
    \item Jinak nedělej nic
    \end{itemize}
\end{enumerate}
\end{figure}
\FloatBarrier

\subsubsection{Tarjanův algoritmus}

Algoritmus hledající silně propojené komponenty pracující v~lineárním čase, efektivnější než Kosarajův.

\begin{figure}[ht]
\onehalfspacing
\begin{enumerate}
\item Označ každý vrchol grafu za~nenavštívený
\item Vytvoř zásobník
\item Vytvoř čítač \emph{i} a nastav ho na~1
\item Vyber nenavštívený vrchol \emph{v} a spusť nad ním podprogram Propoj(\emph{v}):
    \begin{enumerate}
    \item nastav \emph{v.index} a \emph{v.lowlink} na~\emph{i}
    \item zvyš čítač \emph{i} o 1
    \item přidej vrchol \emph{v} do~zásobníku
    \item pro každého navštívitelného souseda \emph{n}:
        \begin{itemize}
        \item pokud \emph{n} nebyl navštíven, nastav \emph{v.lowlink} na~\emph{min(v.lowlink, n.lowlink)}
        \item pokud je \emph{n} v~zásobníku, nastav \emph{v.lowlink} na~\emph{min(v.lowlink, n.index)}
        \end{itemize}
    \item pokud je \emph{v.lowlink} rovný \emph{v.index}:
        \begin{enumerate}
        \item vytvoř propojenou komponentu ze zásobníku
        \item vymaž zásobník
        \end{enumerate}
    \end{enumerate}
\end{enumerate}
\end{figure}
\FloatBarrier

\begin{figure}[ht]
\centering
\includegraphics[width=\textwidth]{images/3_tarjanuv-algoritmus-animace.png}

\caption[Animace Tarjanova algoritmu]{Animace Tarjanova algoritmu\\{\small (LynX, Wikimedia Commons, CC BY-SA 3.0)}}
\end{figure}

\clearpage
\section{Detekce cyklů v~grafu. Eulerova cesta grafem, Hamiltonovská cesta grafem. Floyd-Warhsallův algoritmus. Algoritmus rozpoznání bipartitiního grafu.}

\subsection{Detekce cyklů v~grafu}

\subsection{Eulerova cesta grafem}

\subsection{Hamiltonovská cesta grafem}

\subsection{Floyd-Warshallův algoritmus}

\subsection{Detekce bipartitního grafu}

Graf $G$ je bipartitní právě když je možné jeho vrcholy rozdělit do~dvou množin $V_1$ a~$V_2$ tak že každá hrana spojuje vrchol $V_1$ s~$V_2$.
Bipartitní grafy s~velikostí $|V_1|=m$ a~$|V_2|=n$ se značí $K_\mathrm{m,n}$.

Pro~detekci se využívá BFS:

\begin{enumerate}
\item Vyber bod grafu.
\item Přiřaď mu barvu a~označ ho za~navštívený.
\item Pokud má alespoň jeden sousední bod stejnou barvu jako aktuální bod, graf nelze obarvit dvěma barvami.
\item Všem sousedům přiřaď druhou barvu.
\item Přejdi do~některého ze sousedů který ještě nebyl navštívený.
\item Pokračuj dokud nejsou všechny body grafu obarvené.
\end{enumerate}

\clearpage
\section{Párování grafu, problém maximální shody -- definice, Maďarský algoritmus. Problém časové tabule. Algoritmus barvení grafu. Isomorfismus grafu -- Ullmanův algoritmus.}

% Excentricita vrcholu $x$ grafu $G$ je maximální vzdálenost bodu od~jakéhokoliv bodu v~grafu: $\mathrm{e}(x) = \max\{ d_G(x, y) \ |\ y \in V_G\}$, kde $d_G(x,y)$ je minimální vzdálenost mezi body $x, y$.
% 
% {}Pro~poloměr grafu platí $\mathrm{r}(G) = \min\{ \mathrm{e}(x)\ |\ x \in V \}$.
% \\Pro~průměr grafu platí $\mathrm{d}(G) = \max\{ \mathrm{e}(x)\ |\ x \in V \}$.

% \subsection{Párování grafu}
% 
% Párování grafu je taková podmnožina hran grafu $M \subseteq E$, ve~které žádné dvě hrany nemají společný vrchol.
% Vrcholy, které patří do~párování, se nazývají \emph{saturované}.
% 
% {}\emph{Maximální} párování má nejvíce hran (protože graf může mít párování více).
% \\\emph{Perfektní} párování pokrývá všechny vrcholy grafu.

\subsection{Problém maximální shody}

\subsubsection{Maďarský algoritmus}

Vstupem je matice $m \times n$ (řádky odpovídají \enquote{pracovníkům} a~sloupce \enquote{úkolům}).

\begin{enumerate}
    \item Všem polím v~každém řádku odečti nejnižší hodnotu řádku.
    \item Všem polím v~každém sloupci odečti nejnižší hodnotu sloupce.
    \item Nakresli čáru přes řádky/sloupce tak aby byly překryty všechny nuly co nejmenším počtem čar (= pokrytí).
    \item Pokud je počet čar roven počtu řádků, algoritmus končí.
    \item Najdi nejmenší nepokrytou hodnotu. Odeči její hodnotu od~každého odkrytého řádku a~přidej ji ke~každé nenulové hodnotě v~zakrytému sloupci. Vrať se na~krok~3.
\end{enumerate}

Výsledkem je nějaká možnost z~existujících kombinací nenulových polí.

\subsection{Problém časového rozvrhu}

Ve~škole je $m$ učitelů a~$n$ tříd.
Učitel $i$ musí učit v~třídě $j$ v~semestru $P_{i,j}$.
Problémem je nalezení nejlepšího řešení.

Existuje mnoho verzí tohoto problému:
limitovaný počet učeben,
učitelé mohou učit jen v~určité časy,
žáci musí mít obědovou pauzu,
v~rozvrhu by neměly být zbytečné díry,
\dots

\subsubsection{Barvení grafů}

\paragraph{Largest Degree Ordering}

Vrcholy grafu jsou seřazeny sestupně dle jejich stupně.
Prochází se jimi postupně: vrchol je obarven takovou nejdřívější barvou, která se nevyskytuje u~jeho sousedů.

\paragraph{Incidence Degree Ordering}

Je vybrán vrchol s~nejvyšším stupněm, kterému je přiřazena první barva.
Pro~neobarvené vrcholy je spočítán počet obarvených sousedů a k~obarvení je vybrán ten s~jejich největším počtem.

\subsection{Izomorfismus grafů}

Grafy $G$ a $H$ jsou izomorfní, pokud mezi nimi existuje (hrany zachovávající) bijekce (mapování).

% TODO Způsoby rozpoznání

\subsubsection{Ullmanův algoritmus}
\begin{itemize}
    \item algoritmus pro určení, zda graf G má subgraf G', který by byl isomorfní ke grafu P
    \item pro představu: máme obrázek na krabici s puzzle (G) a chceme vědět, kam konkrétní dílek puzzle (P) zapadá, pokud vůbec
    \item NP-complete problém, protože speciální případ je Hamiltonovský cyklus
\end{itemize}

\clearpage
\section{Definice toku sítí. Problém maximálního toku/minimálního řezu. Ford--Fulkersonův algoritmus.}

% TODO Tok

\subsection{Maximální tok grafem}

Maximální tok a~minimální řez jsou dva výrazy popisující stejný problém, pouze na~něj nahlíží z~jiných stran.

Pracuje se s~orientovaným váženým grafem $G$ se zdrojem $s$ a~cílem $t$, kde pro~každou hranu $e$ platí $c(e) \geq 0$ (kde $c$ je kapacita hrany).

Celkový tok na~vstupu se musí rovnat toku na~výstupu, každý nezdrojový a~neterminální bod musí mít shodnou kapacitu na~vstupu a~výstupu.

Pokud je vstupních/terminálních bodů více, nejsnažší řešení je jejich sloučení do~virtuálního uzlu s~nekonečnou kapacitou.

\subsubsection{Ford--Fulkersonova metoda}

\begin{figure}[ht]
\onehalfspacing
\begin{enumerate}
\item Začni s nulovým tokem
\item Dokud existují rozšiřující cesty:
    \begin{enumerate}
    \item Najdi rozšiřující cestu pomocí BFS
    \item Vypočítej úzké hrdlo pro danou cestu
    \item Pro~každý vrchol $u \rightarrow v$ na cestě
        \begin{enumerate}
        \item Zvyš tok $u \rightarrow v$ o~hodnotu hrdla
        \item Sniž tok $v \rightarrow u$ o~hodnotu hrdla
        \end{enumerate}
    \item Zvyš maximální tok o~hodnotu hrdla
    \end{enumerate}
\end{enumerate}
\end{figure}

FFM není plně specifikována; když se hovoří o~algoritmu, nazývá se Edmonds--Karpovým algoritmem.

\begin{figure}[ht]
\centering
\includegraphics[height=25em]{images/5_edmonds-karp}
\caption[Řešení maximálního proudu Edmonds--Karpovým algoritmem]{Řešení maximálního proudu Edmonds--Karpovým algoritmem\\{\small (Cburnett, Wikimedia Commons, CC BY-SA 3.0)}}
% TODO Cesta z D->E má být E->D
\end{figure}
\FloatBarrier

\subsection{Úzké hrdlo}

% TODO Nutno ověřit.
Úzké hrdlo cesty je minimální hodnota kapacity hrany na~této cestě.

Úzké hrdlo mezi dvěma vrcholy je maximum z~úzkých hrdel přes všechny cesty mezi těmito vrcholy.

Úzké hrdlo grafu je minimum z~úzkých hrdel cest mezi vstupem a~terminálem.

\subsection{Reziduální cesta}

Reziduální kapacita hrany je rozdíl kapacity a~proudu: $c_\mathrm{res}(u,v) = c(u,v) - f(u,v)$.

Reziduální cesta je cesta od~terminálu ke~vstupu vytvořená reziduálními hranami.

\clearpage
\section{Univerzální aproximační teorém. Neuron, Maticová verze dopředné neuronové sítě. Rychlost učení. Dávky a minidávky a efekt na~učení se. Vrstva zahazování.}

\subsection{Univerzální aproximační funkce}

Feedforward neuronová síť je algoritmus inspirovaný biologickýmy neurony.
Univerzální aproximační věta říká, že FFNN s~jedinou skrytou vrstvou obsahující konečný počet neuronů může aproximovat libovolnou kontinuální funkci s~libovolnou přeností.
Neříká však nic o~způsobu konstrukce aktivační funkce.

\subsection{Neuron}

\subsection{Dopředná neuronová síť}

Jde o~nejstarší a~nejjednodušší typ sítí.
FFNN je umělá neuronová síť ve~které neurony netvoří cykly (čímž se liší od~rekurentních neuronových sítí).
Informace prochází od~vstupu přes (volitelné) skryté vrstvy do~vstvy výstupní.

Jednovrstvá síť (ve~které je vstup přímo napojen na~výstup) se nazývá \emph{perceptron}.
Vstupy jsou sečteny na~základě vah a~tvoří výslednou hodnotu (typicky jedinou).
Vzhledem ke~své limitaci se dokáže naučit pouze lineárně separovatelné funkce (mezi které \emph{ne}patří například XOR).
Reálně využívané FFNN však mají skrytých vrstev více.

Vícevrstvé sítě využívají různé způsoby učení a nejpopulárnější z~nich je zpětná propagace (\emph{back-propagation}).
Výstupní hodnoty jsou porovnány se správnou odpovědí a informace je různými způsoby vracena zpátky do~sítě.
Algoritmus takto mění váhy jednotlivých propojení mezi neurony tak aby se celková chyba o~trochu snížila.

Když je tento proces opakován v~dostatečně velkém počtu cyklů, síť konverguje a její chybovost se přestane měnit.
Tento proces se nazývá gradientní sestup.

V~síti také může dojít k~přetrénování: FFNN se \enquote{moc zaměří} na~trénovací sadu a přestane zachycovat opravdové statické vlastnosti datasetu.

\subsection{Maticová reprezentace NN}

\begin{figure}[ht]
    \onehalfspacing
    \centering
    \includegraphics[height=10em]{images/6_maticova-operace}
    $$
    \sigma \left(
    \underbrace{
        \left[ \begin{matrix}
        1 & -2 \\
        -1 & 1 \\
        \end{matrix} \right]
    }_{\mathrm{v\acute{a}hy}}
    \underbrace{
        \left[ \begin{matrix}
        1 \\
        -1 \\
        \end{matrix} \right]
    }_{\mathrm{vstup}} +
    \underbrace{
        \left[ \begin{matrix}
        1 \\
        0 \\
        \end{matrix} \right]
    }_{\mathrm{bias}}
    \right) = \left[ \begin{matrix}
    0.98 \\
    0.12 \\
    \end{matrix} \right]
    $$
    \caption{Ukázka výpočtu hodnot neuronů první skryté vrstvy}
\end{figure}
\FloatBarrier

\subsection{Rychlost učení}

\subsection{Dávky a minidávky a efekt na~učení se}

\subsection{Gradientní sestup}

Cílem gradientního sestupu je nalezení globálního minima v~prostoru všech možných řešení.

Pokud se pracuje s~velkým datasetem (který se nevejde do~VRAM na~GPU), lze ho rozdělit do~menších částí.
Větší členění ale zpomaluje efektivní hledání cesty k~maximu.

Začíná se v~náhodném bodu prostoru řešení $\theta$.
Úpravou vah neuronů se tento bod posouvá směrem k~minimu, tj. k~optimálnímu řešení.
Nic negarantuje že bude globální minimum nalezeno; dle vstupního bodu může funkce konvergovat k~lokálnímu optimu ze~kterého se již nedostane.

Pokud je trénovací krok (\emph{learning rate}) $\eta$ moc velký, může optimum \enquote{přeskočit} a točit se kolem něj. Proto je lepší nechat algoritmus učit pomaleji.

\subsection{Vrstva zahazování}

\emph{Dropout} vrstva zajišťuje že nedojde k~přetrénování určité části sítě které je stimulována nejčastěji.

Při~každé interaci má neuron $p$-procentní pravděpodobnost dočasného vypnutí.
Když je vypnutý, nepodílí se na~výsledku ani mu nejsou upravovány váhy; je tak zajištěno že se síť trénuje rovnoměrně.

\subsection{Aktivační funkce}

Aktivační funkce přináší nelinearitu.

\begin{table}[ht]
\onehalfspacing
\centering
\begin{tabular}{|l|l|}
    název & vzorec \\ \hline \hline
    unit step & $\phi(z) = \begin{cases}
    0 & z < 0, \\
    0.5 & z = 0, \\
    1 & z > 0 \\
    \end{cases}$ \\
    signum & $\phi(z) = \begin{cases}
    -1 & z < 0, \\
    0 & z = 0, \\
    1 & z > 0 \\
    \end{cases}$ \\
    lineární & $\phi(z) = z$ \\
    logistická (sigmoid) & $\phi(z) = \frac{1}{1 + e^{-z}}$ \\
    hyperbolická & $\phi(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$ \\
    ReLU & $\phi(z) = \max(0, z)$ \\
\end{tabular}
\caption{Předpisy některých aktivačních funkcí}
\end{table}
\FloatBarrier

\subsection{Softmax}

Softmax je výstupní vrstva reprezentující kategorickou distribuci.
Každý výstupní neuron má přiřazen svůj význam (např. číslice 0--9) a pravděpodobnost (interval $\left<0, 1\right>$) s~jakou vstup odpovídá tomuto významu.
Suma všech hodnot na~výstupech neuronů se rovná jedné.

\clearpage
\section{Konvoluční neuronové sítě – princip. Max pooling, Dávková normalizace. Známé architektury neuronových sítí.}

CNN jsou inspirovány zrakovým centrem mozku zvířat.
Jednotlivé neurony reagují pouze na~část obrazu, ne na~celek; jejich jádra (\enquote{zorná pole}) se částečně překrývají.
CNN jsou tedy vhodné pro~zpracování dat majících mřízkové uspořádání (hodnoty v~čase (1D), obraz (2D), MRI a~CT (3D)).

Velikost jádra (\emph{kernel size}) je vždy čtverec o~liché velikosti: $3\times3$, $5 \times 5$, $11 \times 11$, \dots

V~konvoluční vrstě se trénují parametry jádra, v~hustě propojené vrstvě váhy.
Každá vrstva má více jader (8, 16, 24, \dots).

Jádra mohou mít různou funkci:
rozpoznání hran $\left[ \begin{matrix}
-1 & -1 & -1 \\
-1 &  8 & -1 \\
-1 & -1 & -1 \\
\end{matrix} \right]$,
ostření $\left[ \begin{matrix}
 0 & -1 &  0 \\
-1 &  5 & -1 \\
 0 & -1 &  0 \\
\end{matrix} \right]$,
\\
rozmazání $\frac{1}{9} \left[ \begin{matrix}
1 & 1 & 1 \\
1 & 1 & 1 \\
1 & 1 & 1 \\
\end{matrix} \right]$,
\dots

Velikost kernelu na~výstupu je menší než na~vstupu:
$\mathbf{X} \mathbf{K} = \left[ \begin{matrix}
0 & 1 & 2 \\
3 & 4 & 5 \\
7 & 8 & 9 \\
\end{matrix} \right] \left[ \begin{matrix}
0 & 1 \\
2 & 3 \\
\end{matrix} \right] = \left[ \begin{matrix}
19 & 25 \\
37 & 43 \\
\end{matrix} \right]$.

\emph{Zero padding} umožňuje výstupní velikost zvětšit:
\\-- vstup ($8 \times 8$) $\rightarrow$ konvoluce ($6 \times 6$);
\\-- vstup ($8 \times 8$) $\rightarrow$ zero padding ($10 \times 10$) $\rightarrow$ konvoluce ($8 \times 8$).

\subsection{Metody regularizace}

\subsubsection{Max pooling}

\begin{figure}[ht]
\onehalfspacing
\begin{minipage}[c]{0.68\textwidth}
    Zmenšení dimenzionality vstupu při~zachování podstatných informací.
    Redukce velikosti zmenšuje výpočetní i~paměťovou náročnost a~snižuje pravděpodobnost \emph{overfittingu} dat.

    % Odstavce se uvnitř 'figure' chovají zvláštně
    \vspace*{1em}
    Overfitting nastává když síť odpovídá moc přesně vstupním datům a pro~data jiná nevrací správné výsledky, protože je moc specializovaná na~trénovací sadu.
\end{minipage}
%\hspace*{0.04\textwidth}
\begin{minipage}[c]{0.30\textwidth}
    $$
    \left[\begin{matrix}
    1 & 1 & 2 & 4 \\
    5 & 6 & 7 & 8 \\
    3 & 2 & 1 & 0 \\
    1 & 2 & 3 & 4 \\
    \end{matrix} \right] \rightarrow \left[\begin{matrix}
    6 & 8 \\
    3 & 4 \\
    \end{matrix} \right]
    $$
    \caption{Max pool $2 \times 2$ zachovávající pouze nejvyšší hodnotu.}
\end{minipage}
\end{figure}
\FloatBarrier

\subsubsection{Dávková normalizace}

Umisťuje se za~lineární váhy (konvoluční vrstvu).
Zvláště v~hlubokých sítích se využívá při~trénování, protože malé rozdíly v~mělčích skrytých vrstvách mohou výrazněji ovlivnit hlubší skryté vrstvy, a zpomalit tak proces učení.

\subsection{Přenesené učení}

% TODO

\subsubsection{Architektury neuronových sítí}

% Více na: https://cs.education-wiki.com/4130807-convolutional-neural-networks

\textbf{LeNet} (1998): 7 vrstev, rozpoznání znaků v~dokumentu.

\textbf{ImageNet}: 8 vrstev, tisíc kategorií s~tisícem obrázků na~kategorii.

\textbf{ZF Net} (2012): popularizovala CNN.

\textbf{GoogleNet} (2014): 22 vrstev

\textbf{VGGNet}: 16 vrstev

\textbf{ResNet} (2015): 152 vrstev, využívá dávkovou normalizaci. Používá se ve všech algoritmech hlubokého učení.

\clearpage
\section{Lineární regrese. Polynomiální regrese. Logistická regrese. Optimalizace s~pomocí gradientního sestupu.}

\subsection{Lineární regrese}

Lineární regrese představuje aproximaci hodnot přímkou.

\begin{table}[h]
	\centering
	\begin{tabular}{ |l|l| }
    Obytná plocha [$\text{m}^2$] & Cena nemovitosti [USD] \\ \hline \hline
    2104 & 400\,000 \\ \hline
    1416 & 232\,000 \\ \hline
    1534 & 315\,000 \\ \hline
	\end{tabular}
	\caption{Ukázka lineární regrese na~cenách nemovitostí}
    \label{tabulka-linearni-regrese}
\end{table}

Vezmeme-li v~úvahu tabulku výše kde $x$ je vstupní proměnná (\emph{feature}) a $y$ je výstup.
Využije se lineární regrese pro~jednu proměnou.

Model této linární regrese lze reprezentovat funkcí zvanou \emph{hypothesis} s~tvarem $h_\theta(x) = \theta_0 + \theta_1 x + \epsilon$
kde $\theta_0$ je bias, $\theta_1$ úhel a $\epsilon$ chyba.

Chybová funkce $J$ má tvar
$$J(\theta_0, \theta_1) = \frac{1}{2m} \sum_{i=1}^{m} (h_\theta (x^{(i)}) - y^{(i)})^2$$
kde $m$ je počet prvků trénovací množiny, $h_\theta(x^{(i)})$ je předpovězená hodnota $y$ a $y^{(i)}$ je reálná hodnota $y$.
Část $(h_0(x^{(i)})-y^{(i)})^2$ znamená minimalizaci čtvercového rozdílu (\emph{square difference}).
Proměná $m$ značí velikost trénovacích dat (v~případě tabulky č.~\ref{tabulka-linearni-regrese} je $m= 3$).

\subsubsection{Výpočet $\theta$}

$\theta$ lze vypočítat pomocí normálové rovnice nebo algoritmem gradientního sestupu.

Pomocí normálové rovnice se optimální hodnota vypočítá jako $\theta = (X^T X)^{-1} X^T y$.
Výhodou je jednoduchá implementace, což je vykoupeno komplexitou O($n^3$).

Jelikož by pro~velký počet $n$ byl výpočet časově náročný, využívá se algoritmu gradientního sestupu:
$\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0, \theta_1)$
kde $j$ je index váhy, $\alpha$ je rychlost učení (např. $10^{-3}$).
Pokud s~každou iterací hodnota $J(\theta)$ neklesá, je nutné $\alpha$ snížit.

\begin{table}[ht]
	\centering
	\begin{tabular}{ |l|l|l|}
    Počet místností & Obytná plocha [$\text{m}^2$] & Cena nemovitosti [USD] \\ \hline \hline
    5 & 2104 & 400\,000 \\ \hline
    7 & 1416 & 232\,000 \\ \hline
    4 & 1534 & 315\,000 \\ \hline
	\end{tabular}
	\caption{Ukázka lineární regrese s~dvěma proměnnými}
    \label{upravena-tabulka-linearni-regrese}
\end{table}

Vezmeme-li v~úvahu tabulku č.~\ref{upravena-tabulka-linearni-regrese}, nepoužijeme lineární regresi pro~jednu proměnou, ale pro~proměných více.
Vše funguje na stejném principu, jen s~menšími změnami ve~vzorcích.

\emph{Hypothesis} funkce se změní na~$h_{\theta}(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \dotsb + \theta_n x_n$.
Minimalizační funkce $J(\theta)$ se nezmění jen počítáme $J(\theta_0, \theta_1, \theta_2, \dots , \theta_n)$.

Hodnoty vstupů $x$ (\emph{features}) se reprezentují pomocí matice $X$. 
Počet $x$ vždy bude odpovídat počtu $\theta$ s~tím, že pro~první $x_0$ bude vždy roven 1 a další $x_n$ budou rovny hodnotám z~tabulky.
Tuto matice lze předzpracovávat třemi způsoby (\emph{preprocessing}):
\begin{itemize}
    \item
    Škálovaní, kdy $x_1$ až $x_n$ upravíme dle vzorce $x = x/x_{\text{max}}$. 
    Hodnota $x_{\text{max}}$ je maximální hodnota v~daném sloupci.

    \item
    Normalizace průměrem, kdy $x_1$ až $x_n$ upravíme dle vzorce $x = (x - \mu)/(x_{\text{max} - x_{\text{min}}})$.
    Hodnota $x_{\text{max}}$ a $x_{\text{min}}$ je maximální a minimální hodnota v~daném sloupci, hodnota $\mu$ je průměr všech hodnot ve~sloupci.

    \item
    Standardizace, kdy $x_1$ až $x_n$ upravíme dle vzorce $x = (x - \mu)/\sigma$.
    Hodnota $\sigma$ je směrodatná odchylka.
\end{itemize}

\subsection{Polynomiální regrese}

Polynomiální regrese je velmi podobná lineární regresi, jen místo přímky tvoříme křivku využívající polynomiální funkci.

Polynomiální regresi má cenu používat pokud máme více vstupů $x$ (\emph{feature}).
Pro \emph{hypothesis} platí $h_0(x) = \theta_0 + \theta_1 x + \theta_2 x^2 + \cdots + \theta_n x^n$.
Rovnice minimalizační funkce a gradientního sestupu se nezmění.

I~v~případě polynomiální regrese je potřeba předzpracovat data.

\subsection{Regrese vs. klasifikace}

Klasifikace je podobná regresi, ale předpovídané hodnoty jsou diskrétní.
Rozděluje se na~binární a multiclass klasifikaci.
U~binární vybíráme ze~dvou možností, zatímco u multiclass se vybírá z~možností několika.

Binární klasifikace využívá regresi, která má ve~většině případů špatné výsledky.
Pro~zlepšení výsledků lze využít logickou regresi.

\subsection{Logická regrese}

Kde o~klasifikační algoritmus používající sigmoid s~hodnotami $\left<0, 1\right>$:
$h_0(x) = g(\theta^T x) = \frac{1}{1+e^{-\theta^T x}}$.

Sigmoid funkce $g(z) = \frac{1}{1+e^{-z}}$ je vykreslena na~obrázku č.~\ref{logisticka-regrese-sigmoid}.

\begin{figure}[ht]
    \onehalfspacing
    \centering
	\begin{tikzpicture}
	    \begin{axis}[
	        height = 15em,
	        width = 30em,
	        axis on top = true,
	        axis x line = bottom,
	        axis y line = left,
	        x axis line style = -,
	        y axis line style = -,
	        tick align = outside,
	        every tick/.append style = {
	            black,
	            thin
	        },
	        % grid = major,
	        ymin = 0,
	        ymax = 1,
	        xlabel = $z$,
            ylabel = $g(z)$,
	    ]
	        \addplot[
	            blue,
	            domain = -5:5,
	            samples = 100
	        ]
	            {1/(1+exp(-x))};
	    \end{axis}
	\end{tikzpicture}
    \caption{Sigmoid.}
    \label{logisticka-regrese-sigmoid}
\end{figure}

Model lze reprezentovat pomocí $h_0(x) = P(y = 1 | x ; \theta)$. 
Tímto získáme pravděpodobnost, že $y$ bude 1 v~závislosti na $x$ a $\theta$
(např. \emph{na~základě velikosti nádoru urči pravděpodobnost že je zhoubný}).
Pro~opačnou hodnotu má rovnice tvar $1 - h_0(x) = P(y = 1 | x ; \theta)$.

Minimalizační funkce má tvar 
$$
J(\theta) = -\frac{1}{m} \sum_{i=1}^{m} \left[ y^{(i)}\log(h_0(x^{(i)})) + (1-y^{(i)})\log(1 - h_0(x^{(i)})) \right]
$$
kde $y^{(i)}\log(h_0(x^{(i)})) = 0$ když $y = 0$ a $(1-y^{(i)})\log(1 - h_0(x^{(i)})) = 0$ když $y = 1$.

Logistická regrese s~využitím gradietního sestupného algoritmu má tvar 
$$
\theta_{i+1} := \theta_i - \frac{\alpha}{m} X^T(g(X\theta_i) - \vec{y})
$$
kde $(g(X\theta_i) - \vec{y})$ určuje chybu (error).

\subsection{Optimalizace s~poocí gradientního sestupu}

% TODO

Zdroje:
\begin{enumerate}
    \item \url{https://www.youtube.com/watch?v=4b4MUYve_U8}
    \item \url{https://www.youtube.com/watch?v=het9HFqo1TQ}
\end{enumerate}

\clearpage
\section{Rekurentní neuronové sítě. LSTM. UNet sítě. Struktury neuronových sítí.}

\subsection{Struktury neuronových sítí}

\begin{figure}[h]
    \centering
	\includegraphics[height=10em]{images/09_NN-struktura.png}
    \caption{Struktura neuronových sítí}
    \label{NNS}
\end{figure}

\textbf{One to one} využívá většina neuronových sítí. 
Vstupem je objekt (obrázek, vektor, \dots) s~pevnou velikostí, poté informace prochází přes skryté vrstvy; poslední vrstva má jednotný výstup.

\textbf{One to many} se využije v~případě kdy máme jeden objekt (obrázek), ale výstupů má být více (jednotlivá slova na~obrázku).

\textbf{Many to one} se využije v~případě kdy máme víc objektů (slova ve~větě, snímky ve~videu) na~vstupu, ale pouze jeden výstup (zda je věta pozitivní nebo negativní, co se děje ve~videu).

\textbf{Many to many (1)} se využije v~případě kdy má být délka vstupu a~výstupu proměnlivá.
Toho se dá využít v~překladu z~jazyků, kdy je vstupem věta a~výstupem věta přeložená.

\textbf{Many to many (2)} se využije v případě proměnlivé délky vstupu a~rozhodnutí pro~každý objekt na~výstupu.
Např. kdybychom měli na~vstupu snímky videa a~pro každý tento snímek bychom chtěli vytvářet nějakou klasifikaci na~výstupu.

\subsection{Rekurentní neuronové sítě (RNN)}

Skladají se ze~tří částí, kterými jsou vstup $X$, výstup $Y$ a buňka RNN.

Vstup $X$ se~předá RNN, který má skrytý interní stav $h$.
Při~každém čtení nového vstupu se tento interní stav aktualizuje a~při dalším čtení vstupu bude  použit pro~učení RNN (\emph{feed the~model}), což způsobí že model je zpožděn o~jeden krok.
Vysledkem je výstup z~RNN buňky.

\begin{figure}[h]
    \centering
	\includegraphics[height=10em]{images/09_RNN.png}
    \caption{Základní princip RNN}
    \label{RNN}
\end{figure}

RNN lze matematicky zapsat jako $h_t = f_W(h_{t-1},x_t)$, kde $f_W$ je určitá funkce, $h_t$ je nový stav, $h_{t-1}$ je předcházející stav a~$x_t$ je vstup $X$.

\subsubsection{Unrolled RNN}

Unrolled RNN je pouze převedení základního principu tak, aby to bylo pochopitelnější, viz obrázek č.~\ref{unrolledRNN}.

\begin{figure}[h]
    \centering
	\includegraphics[height=10em]{images/09_unrolled-RNN.png}
    \caption{Unrolled RNN}
    \label{unrolledRNN}
\end{figure}

\subsection{Long Short Term Memory (LSTM)}

LSTM funguje na stejném principu jako RNN, ale buňka je mnohem složitější.
Obsahuje několik vrstev, které kontrolují proud informací a jsou založené na~několika branách (\emph{gates}).
Tyto brány regulují kolik informací bude propuštěno a~kolik ne.

V~principu lze cyklus LSTM buňky zapsat následovně:
\begin{itemize}
	\item Zapomene nepotřebnou historii
	\item Uloží relevantní části nové informace
	\item Pomocí předchozích dvou kroků aktualizuje vnitřní stav
	\item Vygeneruje výstup
\end{itemize}

\begin{figure}[h]
    \centering
	\includegraphics[height=10em]{images/09_lstm.png}
    \caption{LSTM buňka}
    \label{LSTM}
\end{figure}
\FloatBarrier

\subsection{U-net síťě}

\subsubsection{Semantic segmentation}

Využívá se při rozpoznávání objektů na~obrázku a~rozdělení oblastí pixelů do~kategorií (pes, kámen, strom, \dots).
Pokud máme dve stejné kategorie, semantic segmentation mezi nimi nerozlišuje.

\subsubsection{U-net}

Byl vytvořen pro~biomedicínu, ale dnes se využívá i~jinde.
Je tvořen ze~dvou hlavních částí (cest):
\emph{contraction path}, ve~které se rozlišení vstupu zmenšuje nejčastěji o~polovinu, a
\emph{expansion path}, kde se výstup z~\emph{contraction path} spojuje s~výstupem nižší vrstvy.

\begin{figure}[h]
    \centering
	\includegraphics[height=15em]{images/09_unet.png}
    \caption{U-net architektura}
    \label{LSTM}
\end{figure}

Zdroje:
\begin{enumerate}
    \item \url{https://www.youtube.com/watch?v=SEnXr6v2ifU}
    \item \url{https://www.youtube.com/watch?v=6niqTuYFZLQ}
    \item \url{https://www.youtube.com/watch?v=azM57JuQpQI}
\end{enumerate}


\clearpage
\section{Zpětnovazební učení, Q-učení, Průzkum vs. využití. SARSA.}

\subsection{Zpětnovazební učení}

Zpětnovazební učení maximalizuje výsledek bez toho aby znal způsob jak ho správně dosáhnout.
Využívá algoritmů jako je Q-učení, SARSA, TD-učení a~další.

\subsubsection{Rozdíl oproti genetickým algoritmům}

Hlavním rozdílem je fakt, že zpětnovazební učení se snaží dosáhnout co nejlepšího výsledku, zatímco GA hledá spíše optimální výsledek.
Dalším rozdílem je, že GA výsledek měří dle \emph{fitness}, zatímco zpětnovazební učení dle odměny (\emph{reward}).

\subsection{Markov Decision Process (MDP)}

MDP se skládá ze~čtyř částí a jedné volitelné:
\begin{itemize}
	\item $S$ je množina stavů (nejméně začátek a konec),
	\item $A$ je množina akcí,
	\item \emph{transition state} udávající pravděpodobnost přesunu ze~stavu $s$ do~dalšího stavu provedením akce $a \in A$,
	\item \emph{reward} je odměna a~počítá se stejně jako \emph{transition state}, jen výsledkem není pravděpodobnost, ale odměna (kladná nebo záporná),
	\item (volitelně) $\gamma$ \emph{discount factor} ($0 \leq \gamma \leq 1$).
\end{itemize}

\begin{figure}[h]
    \centering
	\includegraphics[height=10em]{images/10_MDP.png}
    \caption{Princip MDP}
    \label{mdp}
\end{figure}

\subsection{Q-učení}

Q-učení je založeno na~MDP.
Počet možných akcí a~stavů je konečný.

Výběr akce v~závislosti na~aktuálním stavu se vybírá dle Q-value.
Q-value určuje vhodnost (kvalitu) akce v~daném stavu.

Q-values se vypočítají jako odhad dalších odměn ve~zbylých krocích aktuálního příběhu.
Příběh nebo epizoda je jedna iterace učení (pokus).
Takže čím blíže jsme k~cíli tím víc Q-value narůstá.

Q-value se pro~každý stav a~akci zapíše do~Q-table.

\subsubsection{Exploration vs exploitation}

\emph{Exploration} slouží pro~prozkoumání prostředí a~nalezení informací o~něm.

\emph{Exploitation} slouží k~využítí znalostí o~prostředí.

Ve~zpětnovazebním učení se musí docílit optimalizace mezi exploration a~exploitation, a k~této optimalizaci se využívá \emph{Epsilon greedy strategy}.
Ta využívá hodnoty $\epsilon$, která slouží k~určení poměru při~rozhodování.
V~počátečních příbězích je nastaveno $\epsilon = 1$, tj. že bude probíhat pouze exploration.
V~dalších příbězích se $\epsilon$ pomalu snižuje.

\subsubsection{Aktualizace Q-value}

Probíhá dle rovnice
$q^\text{new}(s,a) = (1-\alpha)\, q(s,a) + \alpha \left(R_{t+1} + \gamma\,\text{max}\,q(s',a')\right)$.

Hodnota $\alpha$ je learning rate, který určuje kolik informací z~přechozí Q-value chceme ponechat: $0 \leq\ alpha \leq 1$.
Čím vyšší je tato hodnota tím rychleji se projeví nová hodnota Q-value.

Hodnota $q(s,a)$ určuje předchozí Q-value.

Hodnota $\left(R_{t+1} + \gamma\,\text{max}\,q(s',a')\right)$ určuje momentální (novou) vypočítanou Q-value.

Takže aktualizovaná Q-value je stará hodnota + nová.

\subsubsection{Jak v~krocích funguje Q-learning}

\begin{enumerate}
    \item Inicializace Q-values v~Q-table
    \item Pro~každý příběh
    \begin{enumerate}
    	\item Výběr mezi exploration a~exploitation (náhodná generace čísla)
    	\item Provede se akce
    	\item Aktualizuje se Q-value
    \end{enumerate}
    \item Po~ukončení příběhů je připravená Q-table
\end{enumerate}

\subsection{Q-učení vs SARSA}

SARSA používá politiku (\emph{policy}) chovaní k~výběru další akce.
Q-učení nevyužívá politiku k~výběru další akce, ale odhaduje budoucí výnosy, které aktualizuje.

Zdroje:
\begin{enumerate}
    \item \url{https://www.youtube.com/watch?v=nyjbcRQ-uQ8}
    \item \url{https://www.youtube.com/watch?v=mo96Nqlo1L8}
\end{enumerate}

\clearpage
\section{Metody náhodného průchodu, metoda Node2Vec. Členění druhů umělé inteligence. Pojmy: kombinatorická generalizace, relační induktivní zaměření.}

% \clearpage
% \section{Extrakce znalostí ze stromových a grafových struktur. Metoda náhodného průchodu. Node2Vec. Obecná umělá inteligence – relační induktivní zaměření, kombinatorická generalizace. Předávání zpráv.}
% 
% Klasicky jsou data pro strojové učení uspořádana ve vektoru (1D, 2D). 
% Využívá se k~tomu nejčastěji lineární regrese nebo CNN (konvoluční neuronové sítě). 
% Pokud bychom chtěli převést stromovou nebo grafovou strukturu do~vektrou, bude to možné aproximací a~se ztrátou některých informací.
% Proto se využívá různých metod na~převedení grafové struktury na~zpracovatelné podoby pomocí embedding nodes.
% 
% \subsection{Embeddings nodes}
% 
% Využívá DeepWalk metodu (první metoda). 
% Ukazuje jak by vypadal graf ve~2D, kdy podobné vrcholy jsou v~tomto prostoru blízko při sobě.
% 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[width=\textwidth]{images/11_deepwalk-graf}
%     \caption{DeepWalk metoda}
% \end{figure}
% 
% Cílem je zakódovat vrcholy z~grafu do~embedding prostoru tak že podobnost v~embedding prostoru se blíží podobnosti grafu.
% Tento cíl udává rovnice:
% 
% $$\text{similarity}(u,v)\approx \mathbf{z}_{v}^{T}\,\mathbf{z}_{u}$$
% 
% Máme encoder, který mapuje vrcholy na~embeddings. 
% Poté je fuknce na~výpočet podobnosti (v~rovnici levá strana), která měří podobnost v~originálním grafu (síti).
% Následně je decoder (pravá strana rovnice), který mapuje embeddings na~hodnotu podobnosti (similarity score).
% Nakonec se parametry encoderu optimalizují tak aby platila rovnice.
% 
% \begin{figure}[ht]
%     \centering
%     \includegraphics[height=10em]{images/11_similarity}
%     \caption{Podobnost mezi grafem a embedding prostorem}
% \end{figure}
% 
% \subsection{random-walk embedding}
% 
% Pravděpodobnost, že $u$ a $v$ se společně vyskytnou na~náhodné cestě grafem je rovna nebo přibližná $\mathbf{z}_{v}^{T}\,\mathbf{z}_{u}$.
% 
% \subsubsection{Přoč random-walk}
% 
% {}Expressivity: podobnost vrcholů zahrnujících místní i~high-order neighborhood informace.
% \\Efficiency: musí se uvažovat pouze páry vrcholů, které se společně vyskytují na~random-walk cestě.
% 
% \subsubsection{Postup}
% 
% Nejprve se odhadne pravděpodobnost navštívení vrcholu $v$ na~náhodné cestě grafem začínající v~bodě $u$, využívající některou random-walk strategii. 
% Následně se optimalizuje embeddings k~zakódování random-walk statistik.
% 
% \subsubsection{Optimalizace postup}
% 
% Nejprve se spustí random-walk s~krátkou pevnou délkou, který startuje z~každého vrcholu $u$ v~grafu využívající nějakou random-walk strategii.
% Dále pro každý vrchol $u$ se sesbírá množina navštívených vrcholů pomocí na~cestě (pomocí random-walk strategie) začínajících z~$u$.
% Nakonec se optimalizuje embeddings (pro vrchol $u$ dokážeme předpovědět sousedy v~množině).
% 
% $$\mathcal{L} = \sum_{u\in V} \sum_{v\in N_R (u)} - \log(\frac{\exp(\mathbf{z}_{u}\,\mathbf{z}_{v})}{\sum_{n\in V} \exp(\mathbf{z}_{u}\,\mathbf{z}_{n})})$$
% 
% První suma značí součet přes všechny vrcholy $u$.
% Druhá suma značí součt přes vrcholy $v$ vyskytujících se na~cestě z~vrcholu $u$.
% Obsah logaritmu značí předpovídanou pravděpodobnost společného výskytu $u$ a $v$ na~cestě.
% 
% Cílem je najít $\mathbf{z}_u$ které minimalizuje $\mathcal{L}$.
% 
% 
% \subsubsection{node2vec}
% 
% Node2vec je velice podobný Deepwalk s~tím rozdílem, že jsou jinak vybírany sousední vrcholy.
% Hlavní myšlenkou je kompromis mezi lokálním (BFS) a globálním (DFS) pohledem na~graf (síť).
% Node2vec má dva parametry:
% \begin{itemize}
% 	\item $p$ udávajací hodnotu návratu k~předchozímu vrcholu (return parameter)
% 	\item $q$ udavající \enquote{poměr} mězi BFS a DFS (\enquote{walk away} parameter)
% \end{itemize}
% 
% Při zvolení $q>p$ se algoritmus zaměřuje spíše na~globální pohled (bude směřovat k~BFS průchodu). Při zvolení $q<p$ se algoritmus zaměří spíše na~lokalní pohled (bude směřovat k~DFS).
% 
% \begin{figure}[ht]
%     \centering
% 	\includegraphics[height=10em]{images/11_node2vec-pq}
%     \caption{Prioritizace $p$ nebo $q$ v~node2vec}
% \end{figure}
% 
% Při prochocházení grafem (viz obr.~\ref{pruchod-node2vec}) může dojít ke třem stavům (z~hlediska bodu $w$):
% \begin{itemize}
% 	\item návrat k~předchozímu vrcholu ($s1$),
% 	\item navštívení vedlejšího vrcholu, který má stejnou vzdálenost od~počátečního vrcholu ($s2$)
% 	\item navštívení vzdálenějšího vrcholu ($s3$)
% \end{itemize}
% 
% \begin{figure}
%     \centering
% 	\includegraphics[height=10em]{images/node2vec.png}
%     \caption{Průchod grafem v~node2vec}
%     \label{pruchod-node2vec}
% \end{figure}
% 
% \subsection{Relační induktivní zaměření}
% 
% Definice: Jak algoritmus preferuje jeden model před druhým. 
% 
% Existuje několik modelů:
% \begin{table}[ht]
% \centering
% \caption{Vlastnosti komponent}
% \begin{tabular}{|l|c|c|}
% \hline
% Komponent & Entita & Vazba (relace) \\ \hline \hline
% Plně propojené & Units & All-to-all \\ \hline
% Konvoluční & Grid elements & Local \\ \hline
% Rekurentní & Timesteps & Sequential \\ \hline
% Graph network & Vrchol & Hrana \\ \hline
% \end{tabular}
% \end{table}
% 
% \begin{figure}[ht]
%     \centering
% 	\includegraphics[width=\textwidth]{images/11_RIB}
%     \caption{Přístupy relačního induktivního zaměření}
% \end{figure}
% 
% \subsection{Kombinatorická generalizace}
% 
% Kombinatorická generalizace je vlastnost vytvářet nové rozhraní, predikce a chování z~už známých stavebních bloků.
% 
% Příkladem je vycestování na~nové místo.
% Máme známé postupy -- cestovat letadlem; do~Brna; dát si oběd; v~menze.
% Generalizace je nalezení spojitostí mezi těmito věcmi na~už známých znalostech.
% 
% \subsection{Předávání zpráv}
% \tikzstyle{vertexnocolor}=[circle, draw]
% 
% Mějme graf $G$ (viz obr.~\ref{graf-pro-predavani-zprav}) reprezentovatelný maticí souslednosti $\mathbf{A}$: 
% $\left[ \begin{matrix}
% 0 & 1 & 0 & 0 & 0 \\
% 1 & 0 & 1 & 0 & 0 \\
% 0 & 1 & 0 & 1 & 1 \\
% 0 & 0 & 1 & 0 & 0 \\
% 0 & 0 & 1 & 0 & 0 \\
% \end{matrix} \right]$.
% 
% \begin{figure}[ht]
%     \centering
%     \begin{tikzpicture}
% 		\node[vertexnocolor](a) at (1, -2) {A};
%         \node[vertexnocolor](b) at (3, -1) {B};
%         \node[vertexnocolor](c) at (2, 1) {C};
%         \node[vertexnocolor](d) at (0, 0) {D};
%         \node[vertexnocolor](e) at (1, 2) {E};
            % 
%         \begin{scope}[every path/.style={-}, every node/.style={inner sep=1pt}]
%         	\draw (a) -- node [anchor=east] {} (b);
                   % 
%             \draw (b) -- node [anchor=east] {} (c);
                   % 
%             \draw (c) -- node [anchor=east] {} (d);
%             \draw (c) -- node [anchor=east] {} (e);
% 		\end{scope}
% 	\end{tikzpicture}
%     \caption{Graf $G$.}
%     \label{graf-pro-predavani-zprav}
% \end{figure}
% \FloatBarrier
% 
% Po tomto bodu záleží jakou metodu zvolíme.
% 
% \subsubsection{Součet sousedních vrcholů}
% 
% Zvolíme matici $\mathbf{H_1}$, která bude reprezentovat hodnoty vrcholů:
% $\left[ \begin{matrix}
% 1 \\
% 2 \\
% 3 \\
% 4 \\
% 5 \\
% \end{matrix} \right]$.
% 
% Vynásobíme matici $\mathbf{A}$ maticí $\mathbf{H_1}$ a~získáme předávanou zprávu.
% 
% $$
% \left[ \begin{matrix}
% 0 & 1 & 0 & 0 & 0 \\
% 1 & 0 & 1 & 0 & 0 \\
% 0 & 1 & 0 & 1 & 1 \\
% 0 & 0 & 1 & 0 & 0 \\
% 0 & 0 & 1 & 0 & 0 \\
% \end{matrix} \right]
% \left[ \begin{matrix}
% 1 \\
% 2 \\
% 3 \\
% 4 \\
% 5 \\
% \end{matrix} \right] = 
% \left[ \begin{matrix}
% 0 \cdot 1 + 1 \cdot 2 + 0 \cdot 3 + 0 \cdot 4 + 0 \cdot 5 \\
% 1 \cdot 1 + 0 \cdot 2 + 1 \cdot 3 + 0 \cdot 4 + 0 \cdot 5 \\
% 0 \cdot 1 + 1 \cdot 2 + 0 \cdot 3 + 1 \cdot 4 + 1 \cdot 5 \\
% 0 \cdot 1 + 0 \cdot 2 + 1 \cdot 3 + 0 \cdot 4 + 0 \cdot 5 \\
% 0 \cdot 1 + 0 \cdot 2 + 1 \cdot 3 + 0 \cdot 4 + 0 \cdot 5 \\
% \end{matrix} \right] = 
% \left[ \begin{matrix}
% 2 \\
% 4 \\
% 11 \\
% 3 \\
% 3 \\
% \end{matrix} \right]
% $$
% 
% \subsubsection{Průměr sousedních vrcholů}
% 
% Kromě matice sousednosti se vytvoří matice stupňů vrcholu $\mathbf{D}$:
% $\left[ \begin{matrix}
% 1 & 0 & 0 & 0 & 0 \\
% 0 & 2 & 0 & 0 & 0 \\
% 0 & 0 & 3 & 0 & 0 \\
% 0 & 0 & 0 & 1 & 0 \\
% 0 & 0 & 0 & 0 & 1 \\
% \end{matrix} \right]$
% 
% Matice $\mathbf{D}$ se invertuje ($\frac{1}{x}$) kde $x$ je hodnota z~matice:
% $\left[ \begin{matrix}
% 1 & 0 & 0 & 0 & 0 \\
% 0 & 0.5 & 0 & 0 & 0 \\
% 0 & 0 & 0.33 & 0 & 0 \\
% 0 & 0 & 0 & 1 & 0 \\
% 0 & 0 & 0 & 0 & 1 \\
% \end{matrix} \right]$
% 
% Invertovaná matice se vynásobí s~maticí sousednosti, kde vznikne matice $\mathbf{A_\mathrm{avg}}$.
% 
% $$
% \left[ \begin{matrix}
% 1 & 0 & 0 & 0 & 0 \\
% 0 & 2 & 0 & 0 & 0 \\
% 0 & 0 & 3 & 0 & 0 \\
% 0 & 0 & 0 & 1 & 0 \\
% 0 & 0 & 0 & 0 & 1 \\
% \end{matrix} \right]
% \left[ \begin{matrix}
% 1 & 0 & 0 & 0 & 0 \\
% 0 & 0.5 & 0 & 0 & 0 \\
% 0 & 0 & 0.33 & 0 & 0 \\
% 0 & 0 & 0 & 1 & 0 \\
% 0 & 0 & 0 & 0 & 1 \\
% \end{matrix} \right] =
% \left[ \begin{matrix}
% 0 & 1 & 0 & 0 & 0 \\
% 0.5 & 0 & 0.5 & 0 & 0 \\
% 0 & 0.33 & 0 & 0.33 & 0.33 \\
% 0 & 0 & 1 & 0 & 0 \\
% 0 & 0 & 1 & 0 & 0 \\
% \end{matrix} \right]
% $$
% 
% Tato výsledná matice se vynásobí z~maticí $\mathbf{H_1}$ a získáme předávanou zprávu.
% 
% $$
% \left[ \begin{matrix}
% 0 & 1 & 0 & 0 & 0 \\
% 0.5 & 0 & 0.5 & 0 & 0 \\
% 0 & 0.33 & 0 & 0.33 & 0.33 \\
% 0 & 0 & 1 & 0 & 0 \\
% 0 & 0 & 1 & 0 & 0 \\
% \end{matrix} \right]
% \left[ \begin{matrix}
% 1 \\
% 2 \\
% 3 \\
% 4 \\
% 5 \\
% \end{matrix} \right] = 
% \left[ \begin{matrix}
% 2 \\
% 2 \\
% 3.6 \\
% 3 \\
% 3 \\
% \end{matrix} \right] 
% $$
% 
% \subsubsection{Průměr sousedních vrcholů a sám sebe}
% 
% Do~matice sousednosti doplníme, že každý vrchol sousedí sám se sebou a získáme $\mathbf{\tilde{A}}$:
% $\left[ \begin{matrix}
% 1 & 1 & 0 & 0 & 0 \\
% 1 & 1 & 1 & 0 & 0 \\
% 0 & 1 & 1 & 1 & 1 \\
% 0 & 0 & 1 & 1 & 0 \\
% 0 & 0 & 1 & 0 & 1 \\
% \end{matrix} \right]$.
% 
% Výsledná přenášená zpráva se vypočítá pomocí vzorce
% $\mathbf{\hat{A}} = \mathbf{D}^{-\frac{1}{2}}\mathbf{\tilde{A}}\mathbf{\tilde{D}}^{-\frac{1}{2}}$.
% 
% Zdroje:
% \begin{enumerate}
%     \item \url{https://www.youtube.com/watch?v=Xv0wRy66Big}
%     \item \url{https://www.youtube.com/watch?v=ijmxpItkRjc}
% \end{enumerate}
